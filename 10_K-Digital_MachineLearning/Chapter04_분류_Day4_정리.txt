지도 학습
- 레이블(정답, 결정 클래스, 타깃)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식

분류 (Classification)
- 지도학습의 대표적 유형
- 학습 데이터로 주어진 데이터의 피처와 레이블값(결정값, 클래스값)을
- 머신러닝 알고리즘으로 학습해 모델을 생성하고
- 생성된 모델에 새로운 값(테스트 데이터)이 주어졌을 때 알려지지 않은 레이블 값을 예측하는 것
- 기존 데이터가 어떤 레이블에 속한지
- 알고리즘을 통해 패턴을 인지한 뒤
- 새롭게 관측된 데이터에 대한 레이블 판별

대표적인 분류 알고리즘
- 결정 트리 : 데이터 균일도에 따른 규칙 기반
- 나이브 베이즈 : 베이즈 통계와 생성 모델에 기반
- 로지스틱 회귀 : 독립변수와 종속변수의 선형 관계성에 기반 
- 서포트 벡터 머신 : 개별 클래스 간의 최대 분류 마진을 효과적으로 찾는 방식
- 최소 근접 알고리즘 : 근접 거린 기준
- 앙상블 : 여러 머신러닝 알고리즘 결합

결정 트리 장점
- 매우 쉽고 유연하게 적용될 수 있는 알고리즘
- 데이터 스케일리이나 정규화 등의 사전 가공의 영향이 적음

단점
- 규칙이 많아지면 결정 방식 복잡해지고 과적합 발생
- 예측 성능이 저하될 가능성이 높다

엔트로피 (Entropy)
- 데이터 분포의 불순도(impurity)를 수치화한 지표
- 서로 다른 데이터가 섞여 있으면 엔트로피가 높고
- 같은 값이 섞여 있으면 엔트로피 낮음
- 엔트로피를 통해 불순도 판다
- 수치 0 : 모든 값이 동일 (분류하지 않아도 됨)
- 수치 1 : 불순도 최대
    
정보 이득(Information Gain) 지수
- 분류를 통해 정보에 대해 얻은 지식
- 1-엔트로피 지수
- 결정트리는 정보 이득 지수로 분할 기준을 정함
- 즉, 정보 이득 지수가 높은 속성을 기준으로 분할

지니 계수
- 불순도를 수치화한 지표
- 경제학에서 불평등 지수를 나타낼 때 사용하는 계수
- 0이 가장 평등하고, 1로 갈수록 불평등
- 머신러닝에 적용될 때는 지니 계수가 낮을수록
- 데이터 균일도가 높은 것으로 해석
- 지니 계수가 낮은 속성을 기준으로 분할

결정 트리 알고리즘에서 지니 계수 이용
- 사이킷런의 DecisionTreeClassifier 클래스는
- 기본으로 지니 계수를 이용해서 데이터 세트 분할
- 데이터 세트를 분할하는 데 가장 좋은 조건
- 정보 이득이 높거나 지니 계수가 낮은 조건


결정 트리 알고리즘에서 분류를 결정하는 과정
- 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서
- 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤
- 데이터가 모두 특정 분류에 속하게 되면
- 분할을 멈추고 분류 결정

### 결정 트리 하이퍼 파라미터
- 규칙 트리는 규칙 생성 로직을 미리 제어하지 않으면
- 완벽하게 클래스 값을 구별해 내기 위해
- 트리 노드를 계속해서 만들어가기 때문에
- 매우 복잡한 규칙 트리가 만들어져
- 모델이 쉽게 과적합 되는 문제 발생
- 하이퍼 파라미터를 사용하여
- 복잡한 트리가 생성되지 않도록 제어


# max_depth=8일 경우 정확도 : 0.8707로 가장 높음
# 8을 넘어가면 정확도 계속 감속
# 위의 GridSearchCV 예제처럼 깊이가 깊어질수록 테스트 데이터 세트의 정확도 떨어짐

# 결정 트리의 깊이가 길어질수록 과적합 영향력이 커지므로
# 하이퍼 파라미터를 이용해서 깊이 제어 필요
# 복잡한 모델보다 트리 깊이를 낮춘 단순한 모델이 더 효과적인 결과를 산출할 수 있음


앙상블 학습을 통한 분류
- 여러 개의 분류기(Classifier)를 사용해서 에측 결합함으로써
- 보다 정확한 최종 예측을 도출하는 기법
- 단일 분류기 사용 때보다 신뢰성이 높은 예측값을 얻을 수 있음
- 쉽고 편하면서도 강력한 성능 보유
- 대부분의 정형 데이터 분류 시 뛰어난 성능을 나타냄
- 이미지, 영상, 음성 등의 비정형 데이터 분류 : 딥러닝 성능이 뛰어남



대표적인 앙상블 알고리즘
- 랜덤 포레스트
- 그레디언트 부스팅

앙상블 알고리즘 변화
- 뛰어난 성능, 쉬운 사용, 다양한 활용도로 인해 많이 애용되었고
- 부스팅 계열의 앙상블 알고리즘의 인기와 강세가 계속 이어져
- 기존의 그레디언트 부스팅을 뛰어넘는 새로운 알고리즘 가속화

최신 앙상블 알고리즘
- XGBoost
- LightGBM : XGBoost와 예측 성능 유사하면서도 수행 속도 훨씬 빠름
- Stacking  : 여러 가지 모델의 결과를 기반으로 메타 모델 수립
- XGBoost, LightGBM과 같은 최신 앙상블 알고리즘 한 두 개만 잘 알고 있어도
- 정형 데이터의 분류 또는 회귀 분야에서 예측 성능이 매우 뛰어난 모델을 쉽게 만들 수 있음


앙상블 학습 유형
- 보팅(Voting)
- 배깅(Bagging)
- 부스팅 (Boosting)
- 스태킹 등


보팅(Voting) : 여러 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
- 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합
    
배깅(Bagging) : 보팅과 동일하게 여러 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
- 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만
- 샘플링을 서로 다르게 하면서 학습 수행
- 대표적인 배깅 방식 : 랜덤 포레스트 알고리즘

부스팅 (Boosting)
- 여러 개의 분류기가 순차적으로 학습을 수행하되
- 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록
- 다음 분류기에게는 가중치(weight)를 부여하면서
- 학습과 예측을 진행하는 방식
- 예측 성능이 뛰어나 앙상블 학습 주도
- boost : 밀어 올림

스태킹
- 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서
- 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방식

하드 보팅 (Hard Voting)
- 다수결 원칙과 유사
- 예측한 결과값들 중에서 다수의 분류기가 결정한 예측값을
- 최종 보팅 결과값으로 선정

소프트 보팅 (Soft Voting)
- 분류기들이 레이블 값 결정 확률을 평균내서
- 확률이 가장 높은 레이블 값을
- 최종 보팅 결과값으로 선정
- 일반적으로 소프트 보팅이 예측 성능이 좋아서 더 많이 사용

학습/예측/평가 후 성능 비교
- 로지스틱 회귀와 KNN을 보팅방식으로 결합한 것
- 로지스틱 회귀
- KNN

# 보팅 분류기의 정확도가 조금 높게 나타났는데
# 보팅으로 여러 개의 분류기를 결합한다고 해서
# 무조건 예측 성능이 향상되지는 않음

# 그래도 보팅, 배깅, 부스팅 등의 앙상블 방법은
# 전반적으로 다른 단일 ML 알고리즘 보다 어느 정도 뛰어난 예측 성능을 가지는 경우가 많음

# 고정된 데이터 세트에서
# 단일 ML 알고리즘이 뛰어난 성능을 발휘하더라도
# 현실 세계는 다양한 변수와 예측이 어려운 규칙으로 구성되어 있기 때문에
# 다양한 관점을 가진 알고리즘이 서로 결합해서
# 더 나은 성능을 실제 환경에서 이끌어 낼 수 있음

부트 스트래핑 분할 방식
- 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식
- 각 샘플링된 데이터 내에는 중복 데이터 포함










