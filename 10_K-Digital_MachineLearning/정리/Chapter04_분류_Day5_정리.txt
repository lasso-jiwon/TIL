GBM (Gradient Boosting Machine) : 그래디언트 부스트
- 에이다 부스트와 유사하지만
- 가중치 업데이트를 경사 하강법을 이용하는 것이 큰 차이
    - 반복 수행을 통해 오류를 최소화할 수있도록
    - 가중치의 업데이트 값을 도출
    - 오류값 = 실제값 - 예측값
- 분류와 회귀 둘 다 가능


GBM은 수행 시간이 오래 걸린다는 단점이 있지만
과적합에도 강해서 예측 성능이 뛰어난 알고리즘
많은 알고리즘이 GBM을 기반으로 해서 새롭게 만들어 지고 있음

머신러닝 세계에서 가장 각광을 받는 그래디언트 부스팅 기반 ML 패키지
- XGBoost
- LightGBM


XGBoost
- 트리기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중의하나
- 압도적인 수치 차이는 아니지만
- 분류에 있어서 일반적으로 다른 머신러닝 보다 뛰어난 예측 성능을 나타냄
- GBM에 기반하지만, GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제 해결

XGBoost 장점
- 뛰어난 예측 성능
- GBM 대비 빠른 수행 시간 
- 과적합 규제
- 가지치기 (pruning)
- 교차 검증 내장
- 결손값 자체 처리

XGBoost 조기 중단 기능 (Early Stopping)
- 지정한 수만큼의 부스팅 반복 작업이 종료되지 않더라도
- 예측 오류가 더이상 개선되지 않으면
- 중간에 중지해서 수행 시간 개선
- 즉, 오류 감소 효과 없으면 멈춤
- 학습 시간 단축. 특히 최적화 튜닝 단계에서 적절하게 사용 가능

LightGBM
- XGBoost와 함께 부스팅 계열 알고리즘에서 가장 각광을 받고 있음
- 뛰어난 알고리즘이지만 학습 시간이 많이 빠른 편은 아님
- 가장 큰 장점 : XGBoost보다 학습 시간이 작다는 점
    - 메모리 사용량도 적고
    - 그런데 성능은 별 차이 없고
    - 기능은 좀 더 다양하고
- 한 가지 단점 : 적은 데이터 셋에 적용할 경우 과적합이 발생하기 쉽다는 것
    - 약 10,000 건 이하 정도(공식 문서에 기술되어 있음)
- 트리 기반이지만 균형 트리 분할 방식 사용
    - 최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에
    - 트리 깊이가 최소화될 수 있음

Feature Engineering
- 피처들 중 어떤 것이 모델링할 때 중요한 것인지 결정하는 과정
- 모델 성능에 미치는 영향이 크기 때문에 머신러닝 응용에 있어서 상당히 중요한 단계
- 전문성과 시간, 비용이 많이 드는 작업
- PCA 방식 사용

PCA (Pricipal Component Analysis) : 주성분 분석
- 대표적인 차원 축소 알고리즘
- 여러 변수 간 존재하는 상관관계를 이용해
- 대표하는 주성분을 추출해서 차원을 축소하는 기법
- 간단하게 가장 높은 분산을 가지는 데이터의 축을 찾아 차원을 축소


차원 축소
- 많은 피처로 구성된 다차원 데이터 셋을
- 차원을 축소하여 새로운 차원의 데이터 셋을 생성하는 것


Feature Engineering 주요 작업
- 주요 Feature의 데이터 분포도 변경 : Log 변환
- 이상치 제거
- SMOTE 오버 샘플링

Log 변환
- 왜곡된 분포도를 가진 데이터 셋을
- 비교적 정규 분포에 가깝게 변환하는
- 피처 엔지니어링 방식

분포도가 왜곡되어 있다면 해결 방법은 로그 변환이 답
일반 수치 10, 100, 1000하면 10과 100 사이가 크지만
로그로하게 되면 log(10)은, log(100)은 2처럼
간극이 일정하기 때문에 
로그 변환을 이용하면 왜곡된 분포도를 쉽게 정규 변환 형태로 변경 가능

IQR(Inter Qunatile Range)를 이용한 이상치 제거

SMOTE (Synthetic Minority Over_sample Technique)
- 적은 데이터 세트에 있는 개별 데이터들의
- K 최근접 이웃을 찾아서
- K개 이웃들과의 차이를 일정 값으로 만들어서
- 기존 데이터와 약간 차이가 나는 새로운 데이터 생성하는 방식

신용카드 사기 검출 과정
(1) 데이터 일차 가공 및 모델 학습/예측/평가
(2) 중요 데이터 분포도 변환 후 모델 학습/예측/평가
(3) 이상치 데이터 제거 후 모델 학습/예측/평가
(4) SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가


각 케이스 별로 예측 성능 변화 확인
Amount 칼럼에 대해  로그 변환 후
- 로지스틱 회귀 : 정밀도 약간 좋아졌고, 재현율 동일
- LightGBM : 정밀도, 재현율 등 약간씩 성능 향상

이상치 제거 후 가장 성능이 좋아졌음 
- 사긴 데이터 건수 전체에서 4건 지웠음(?)
- 실제로 이상 제거는 효과는 가장 크다고 볼 수 있음 

SMOTE 오버 샘플링 결과
- 재현율이 좋아짐
- 로지스틱 회귀 : 정밀도가 많이 떨어짐
- LightGBM : 정밀도를 희생하더라도 재현율은 향상된 모델이 되었음
    
LightGBM의 경우 전반적으로 향상되었음



