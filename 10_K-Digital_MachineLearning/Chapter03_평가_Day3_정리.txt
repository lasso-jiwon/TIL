분류의 성능 평가 지표
- 정확도 (Accuracy)
- 오차행렬 (Confusion Matrix)
- 정밀도 (Precision)
- 재현율 (Recall)
- F1 Score
- ROC AUC

이진/멀티 분류에 모두 적용되는 지표이지만  
이진 분류에서 더 중요하게 강조되는 지표


단순한 알고리즘으로 예측을 하더라도
데이터의 구성에 따라 정확도 결과는 83.24%로
상당히 높은 수치가 나올 수 있기에
정확도를 평가 지표로 사용할 때는 매우 신중해야 함
특히, 정확도는 불균형한 레이블 값 분포에서 ML 모델의 성능을 판단할 경우
정확한 지표가 아님



오차행렬 (Confusion Matrix) (혼동 행렬)
- 분류의 성능을 평가하는 행렬
- 실제로 참(True)인지 거짓(False)인지,
- 예측을 긍정(Positive)으로 했는지, 부정(Negative)로 했는지에 따라
- 4개의 경우의 수로 구분한 표
- 4분면 행렬에서 예측 클래스와 실제 클래스의 값 유형에 따라 TN, FP, FN, TP 형태
- 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고(confused) 있는지도 함께 보여주는 지표


머신러닝에서 오차행렬이 중요한 이유
- 머신러닝 모델의 예측이 얼마나 잘 한 예측인지를 판단하는 중요한 기준을 제공
- 오차행렬로부터 머신러인 모델의 우수성을 평가하는 다른 여러 지표들을 도출


일반적으 불균형한 레이블 클래스 가지는 이진 분류 모델에서는
- 많은 데이터 중에서 중점적으로 찾아야 하는 아주 적은 수의 결과값에 Positive를 설정해서 1 값을 부여하고
- 그렇지 않은 경우는 Negative로 0을 부여하는 경우가 많음

예1 : 사기 행위 예측 모델
- 사기 행위 : Positive 양성으로 1 값 부여
- 정상 행위 : Negative 음성으로 0 값 부여
    
예2 : 암 검진 모델
- 양성 : Positive 양성으로 1 값 부여
- 음성 : Negative 음성으로 0 값 부여



정밀도(Precision)와 재현율(Recall)
- Positive 데이터 세트의 예측 성능에 더 초점을 맞춘 평가 지표


정밀도 = TP / (FP + TP)
- 예측을 Positive로 한 대상 중에
- 예측과 실제 값이 Positive로 일치한 데이터의 비율
- 예측한 양성 대 맞춘 양성
- 공식의 분모인 (FP + TP)는 예측을 Positive로 한 모든 데이터 건수 (예측한 양성)
- 분자인 TP는 예측과 실제값이 Positive로 일치한 데이터 건수 (맞춘 양성)
- Positive 예측 성능을 더 정밀하게 측정하기 위한 평가 지료
- 양성 예측도라고도 함


재현율 = TP/ (FN + TP)
- 실제값이 Positive인 대상 중에
- 예측과 실제 값이 Positive로 일치한 비율
- 실제 양성 대 예측한(맞춘) 양성 비율
- 공식의 분모인 (FN + TP)는 실제값이 Positive인 모든 데이터 건수(실제 양성)
- 분자인 TP는 예측과 실제값이 Positive로 일치한 데이터 건수(맞춘 양성)
- 민감도(Sensitivity) 또는 TPR(True Positive Rate)


재현율이 상대적으로 더 중요한 지표인 경우
- 실제 Positive 양성 데이터 예측을 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우


정밀도가 상대적으로 더 중요한 지표인 경우
- 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우


재현율과 정밀도의 보완적 관계
- 재현율과 정밀도 무도 TP를 높이는데 동일하게 초점을 맞추지만
- 재현율은 FN(실제 Positive, 예측 Negative)을 낮추는데 초점을 맞추고
- 정밀도는 FP(실제 Negative, 예측 Positive)를 낮추는데 초점을 맞출

- 이 같은 특성 때문에 재현율과 정밀도는 서로 보완적인 지표로
- 분류의 성능을 평가하눈데 적용

- 가장 좋은 성능 평가는 재현율과 정밀도 모두 높은 수치를 얻는 것
- 반면에 둘 중 어는 한 평가 지표만 매우 높고,
- 다른 수치는 매우 낮은 결과를 나타내는 경우는 바람직하지 않음


정밀도 / 재현율 트레이드 오프 (Trade-off)
- 업무에 따라 정밀도/재현율 중요도 다름
- 분류하려는 업무 특성사 정밀도 도는 재현율이 특별히 강조되어야 할 경우
- 분류의 결정 임계값(Threshold)을 조정해서
- 정밀도 도는 재현율의 수치를 높일 수 있음
- 그러나 정밀도와 재현율은 상호 보완적인 평가 지표이기 때문에
- 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지는데
- 이를 정밀도/재현율의 트레이드 오프라고 함

정밀도와 재현율 조합
- Positive 예측의 임계값에 따라 정밀도와 재현율 수치가 변경
- 임계값은 업무 환경에 맞춰 정밀도와 재현율의 수치를 상호 보완할 수 있는 수준에서 적용
- 단순히 하나의 성능 지표 수치를 높이기 위한 수단으로 사용돼서는 안됨

- 정밀도 또는 재현율 중 하나에 상대적인 중요도를 부여해
- 각 예측 상황에 맞는 분류 알고리즘율 튜닝할 수는 있지만
- 정밀도와 재현율 성능 수치를 어느 한쪽만 참조하면 극단적인 수치 조작 가능

F1 Score
- 정밀도와 재현율을 결합한 지표
- 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치를 나타낼때
- 높은 값을 가짐

ROC 곡선  
- FPR(False Positive Rate)이 변할 때 TPR(True Positie Rate)이 어떻게 변하는지를 나타내는 곡선
- 양성으로 잘못 판단한 것에 대한 진짜 양성의 비율을 나타내는 곡선





